diff --git a/preddrl_td3/README.md b/preddrl_td3/README.md
index 7f497b8..5fd84c6 100644
--- a/preddrl_td3/README.md
+++ b/preddrl_td3/README.md
@@ -1,14 +1,17 @@
 # preddrl_TD3
 
-**1.Dependencies**  
+**1.Create Virtual Environment**  
 ``` 
 conda env create -f environment.yaml  
 ``` 
-**2.Run**  
+**2. Activate Virtual Environment and Run**  
 _You must start the simulation environment before you can run the following commands. The simulation environment and DRL algorithm run in different environments_
 ``` 
-conda activate tf2  #launch virtual environment
-python ~/preddrl_td3/src/tf2rl/examples/run_td3.py   #run the script
+#launch virtual environment 
+conda activate tf2 
+pip install -r requirements.txt
+cd preddrl_td3 # change to root dir
+python scripts/run_td3.py   #run the script
 ``` 
 
 **3.Notice**  
diff --git a/preddrl_td3/environment.yaml b/preddrl_td3/environment.yaml
index d5fe418..ad64933 100644
--- a/preddrl_td3/environment.yaml
+++ b/preddrl_td3/environment.yaml
@@ -55,70 +55,5 @@ dependencies:
   - wheel=0.34.2=py36_0
   - xz=5.2.5=h7b6447c_0
   - zlib=1.2.11=h7b6447c_3
-  - pip:
-    - absl-py==0.9.0
-    - astor==0.8.1
-    - cachetools==4.1.0
-    - catkin-pkg==0.4.17
-    - catkin-tools==0.4.5
-    - chardet==3.0.4
-    - cloudpickle==1.1.1
-    - cpprb==8.4.2
-    - decorator==4.4.2
-    - defusedxml==0.6.0
-    - distro==1.5.0
-    - docutils==0.16
-    - empy==3.3.4
-    - future==0.18.2
-    - gast==0.2.2
-    - google-auth==1.14.1
-    - google-auth-oauthlib==0.4.1
-    - google-pasta==0.2.0
-    - grpcio==1.28.1
-    - gym==0.17.1
-    - h5py==2.10.0
-    - idna==2.9
-    - joblib==0.14.1
-    - keras-applications==1.0.8
-    - keras-preprocessing==1.1.0
-    - markdown==3.2.1
-    - msgpack==1.0.0
-    - netifaces==0.10.9
-    - numpy==1.18.3
-    - oauthlib==3.1.0
-    - opencv-python==4.2.0.34
-    - opt-einsum==3.2.1
-    - osrf-pycommon==0.1.9
-    - pandas==1.0.4
-    - protobuf==3.11.3
-    - pyasn1==0.4.8
-    - pyasn1-modules==0.2.8
-    - pyglet==1.5.0
-    - pytz==2020.1
-    - pyyaml==5.3.1
-    - requests==2.23.0
-    - requests-oauthlib==1.3.0
-    - rosdep==0.19.0
-    - rosdistro==0.8.1
-    - rosinstall==0.7.8
-    - rosinstall-generator==0.1.19
-    - rospkg==1.2.4
-    - rsa==4.0
-    - scipy==1.4.1
-    - tensorboard==2.0.2
-    - tensorflow==2.0.0
-    - tensorflow-estimator==2.0.1
-    - tensorflow-gpu==2.0.0
-    - tensorflow-probability==0.8.0
-    - termcolor==1.1.0
-    - tf2rl==0.1.12
-    - tianshou==0.2.2
-    - torch==1.5.0
-    - trollius==2.1.post2
-    - urllib3==1.25.9
-    - vcstools==0.1.42
-    - werkzeug==1.0.1
-    - wrapt==1.12.1
-    - wstool==0.1.17
 prefix: /home/ros_admin/anaconda3/envs/tf2
 
diff --git a/preddrl_td3/gazebo_env/__init__.py b/preddrl_td3/gazebo_env/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/preddrl_td3/gazebo_env/__pycache__/environment_stage_2.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/environment_stage_2.cpython-36.pyc
deleted file mode 100644
index 36a165f..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/environment_stage_2.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/environment_stage_3.cpython-36.pyc
deleted file mode 100644
index 231cfd0..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk.cpython-36.pyc
deleted file mode 100644
index 16cec45..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk2.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk2.cpython-36.pyc
deleted file mode 100644
index a855c8b..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk2.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk_PPO.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk_PPO.cpython-36.pyc
deleted file mode 100644
index 5d275d3..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/environment_stage_3_bk_PPO.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/__pycache__/respawnGoal.cpython-36.pyc b/preddrl_td3/gazebo_env/__pycache__/respawnGoal.cpython-36.pyc
deleted file mode 100644
index 8f0a21d..0000000
Binary files a/preddrl_td3/gazebo_env/__pycache__/respawnGoal.cpython-36.pyc and /dev/null differ
diff --git a/preddrl_td3/gazebo_env/environment_stage_3_bk.py b/preddrl_td3/gazebo_env/environment_stage_3_bk.py
deleted file mode 100644
index da3e91c..0000000
--- a/preddrl_td3/gazebo_env/environment_stage_3_bk.py
+++ /dev/null
@@ -1,261 +0,0 @@
-import rospy
-import numpy as np
-import math
-from gym import spaces
-from gym.utils import seeding
-from math import pi
-from geometry_msgs.msg import Twist, Point, Pose, PoseStamped
-from sensor_msgs.msg import LaserScan
-from nav_msgs.msg import Odometry
-from std_srvs.srv import Empty
-# from tf.transformations import euler_from_quaternion, quaternion_from_euler
-from .respawnGoal import Respawn
-
-
-class Env:
-    def __init__(self):
-        self.goal_x = 1.0
-        self.goal_y = 0
-        self.inflation_rad = 0.37  # 包含0.17的自身半径
-        self.heading = 0
-        self.pre_heading = 0
-        self.max_v = 0.2
-        self.max_w = 1.5
-        self.goal_threshold = 0.3#0.4
-        self.collision_threshold = 0.15
-        self.vel_cmd = [0., 0.]
-        self.initGoal = True
-        self.get_goalbox = False
-        self.position = Pose()
-        self.test = False
-        self.num_beams = 20  # 激光数
-        low = np.array([-2., -2.])
-        high = np.array([2., 2.])
-        self.action_space = spaces.Box(low, high, dtype=np.float32)
-        low = [0.0] * (self.num_beams)
-        low.extend([0., -2., -2*pi, 0])  #极坐标
-        # low.extend([0., -1.5, -2.0, -2.0,-2.0, -2.0]) #笛卡尔坐标
-        high = [3.5] * (self.num_beams)
-        high.extend([0.2, 2., 2*pi, 4])
-        # high.extend([0.2, 1.5, 2.0, 2.0, 2.0, 2.0])
-        self.observation_space = spaces.Box(np.array(low), np.array(high), dtype=np.float32)
-#
-        self.input_shape = 20
-        self.window_size = 3
-#        
-        self.pub_cmd_vel = rospy.Publisher('cmd_vel', Twist, queue_size=5)
-        self.sub_odom = rospy.Subscriber('odom', Odometry, self.getOdometry)
-        self.reset_proxy = rospy.ServiceProxy('gazebo/reset_simulation', Empty)
-        self.unpause_proxy = rospy.ServiceProxy('gazebo/unpause_physics', Empty)
-        self.pause_proxy = rospy.ServiceProxy('gazebo/pause_physics', Empty)
-
-        # self.point_goal = rospy.Subscriber('/move_base_simple/goal', Odometry, self.getOdometry)
-
-        self.respawn_goal = Respawn()
-        self.past_distance = 0.
-#
-        # self.preprocessor = HistoryPreprocessor(self.input_shape, history_length=self.window_size)
-#        
-    def euler_from_quaternion(self, orientation_list):
-        x, y, z, w = orientation_list
-        r = math.atan2(2 * (w * x + y * z), 1 - 2 * (x * x + y * y))
-        p = math.asin(2 * (w * y - z * x))
-        y = math.atan2(2 * (w * z + x * y), 1 - 2 * (z * z + y * y))
-
-        return r, p, y
-
-
-    def getGoalDistace(self):
-        goal_distance = round(math.hypot(self.goal_x - self.position.x, self.goal_y - self.position.y), 2)
-        # goal_distance = round(math.hypot(self.goal_x - self.position.position.x, self.goal_y - self.position.position.y), 2)
-        self.past_distance = goal_distance
-        return goal_distance
-
-    def getOdometry(self, odom):
-        self.position = odom.pose.pose.position
-        orientation = odom.pose.pose.orientation
-        orientation_list = [orientation.x, orientation.y, orientation.z, orientation.w]
-        _, _, yaw = self.euler_from_quaternion(orientation_list)
-
-        # goal_angle = math.atan2(self.goal_y - self.position.y, self.goal_x - self.position.x)
-        # goal_angle = math.atan2(self.goal_y - self.position.position.y, self.goal_x - self.position.position.x)
-        
-        inc_y = self.goal_y - self.position.y
-        inc_x = self.goal_x - self.position.x
-        goal_angle = math.atan2(inc_y, inc_x)
-
-        # if inc_y>0 and inc_x<0:
-        #     goal_angle += pi
-        # elif inc_y<0 and inc_x<0:
-        #     goal_angle -= pi
-        
-        heading = goal_angle - yaw
-
-        if heading > pi:
-            heading -= 2 * pi
-
-        elif heading < -pi:
-            heading += 2 * pi
-
-        self.heading = round(heading, 2)
-        # print(0)
-        # print(goal_angle)
-
-    def getState(self, scan):
-        scan_range = []
-        scan_range_collision = []
-        heading = self.heading
-        done = False
-
-        # for i in range(0,20):
-        #     if scan.ranges[i] == float('Inf'):
-        #         scan_range.append(3.5)
-        #     elif np.isnan(scan.ranges[i]):
-        #         scan_range.append(0)
-        #     else:
-        #         scan_range.append(scan.ranges[i])
-
-        for i in range(len(scan.ranges)):
-            if scan.ranges[i] == float('Inf'):
-                scan_range_collision.append(3.5)
-            elif np.isnan(scan.ranges[i]):
-                scan_range_collision.append(0)
-            else:
-                scan_range_collision.append(scan.ranges[i])
-
-        # obstacle_min_range = round(min(scan_range), 2)
-        # obstacle_angle = np.argmin(scan_range)
-        if self.collision_threshold > min(scan_range_collision) > 0:
-            done = True
-
-        current_distance = round(math.hypot(self.goal_x - self.position.x, self.goal_y - self.position.y), 2)
-        # current_distance = round(math.hypot(self.goal_x - self.position.position.x, self.goal_y - self.position.position.y), 2)
-        if current_distance < self.goal_threshold:
-            self.get_goalbox = True
-            
-        # print(scan_range_collision)
-#        
-        state = scan_range_collision+ self.vel_cmd + [heading, current_distance] # 极坐标
-#
-        # state = scan_range + self.vel_cmd + [self.position.x, self.position.y, self.goal_x, self.goal_y] #笛卡尔坐标
-        
-        return state, done, self.get_goalbox
-       
-    def setReward(self, state, done):
-
-        current_distance = round(math.hypot(self.goal_x - self.position.x, self.goal_y - self.position.y), 2)
-        # current_distance = round(math.hypot(self.goal_x - self.position.position.x, self.goal_y - self.position.position.y), 2)
-        distance_rate = (self.past_distance - current_distance)
-        self.past_distance = current_distance
-
-        if done:
-            rospy.loginfo("Collision!!")
-            reward = -150
-            self.pub_cmd_vel.publish(Twist())
-
-        elif self.get_goalbox:
-            rospy.loginfo("Goal!!")
-            reward = 200
-            self.pub_cmd_vel.publish(Twist())
-            self.goal_x, self.goal_y = self.respawn_goal.getPosition(True, delete=True, test=self.test)
-            # send_goal=None
-            # while send_goal is None:
-            #     try:
-            #         send_goal = rospy.wait_for_message('/move_base_simple/goal', PoseStamped,timeout=5)
-            #     except:
-            #         pass
-            # self.goal_x, self.goal_y = send_goal.pose.position.x, send_goal.pose.position.y
-            # print(send_goal)
-            # rospy.loginfo("Goal position : %.1f, %.1f", self.goal_x,
-            #                   self.goal_y)
-            self.goal_distance = self.getGoalDistace()
-            self.get_goalbox = False
-        else:
-            reward = (self.goal_threshold-state[-1]) * 0.1 #- 0.25*abs(state[-3])15*distance_rate
-            # reward = 15*distance_rate
-        # 增加一层膨胀区域，越靠近障碍物负分越多
-        obstacle_min_range = round(min(state[:self.num_beams]), 2)
-        if obstacle_min_range < self.inflation_rad:
-            # reward += 100.0*(obstacle_min_range - self.inflation_rad)/self.inflation_rad
-            reward -= 5.0*(1 - obstacle_min_range/self.inflation_rad)
-
-        return reward
-
-    def seed(self, seed=None):
-        # 产生一个随机化时需要的种子，同时返回一个np_random对象，支持后续的随机化生成操作
-        self.np_random, seed = seeding.np_random(seed)
-        return [seed]
-
-    def step(self, action):
-        self.pre_heading = self.heading
-        vel_cmd = Twist()
-        # vel_cmd.linear.x = (action[0]*2.5 + 5.0) / 20.0
-        # vel_cmd.linear.x = (action[0]*1.75 + 3.5) / 20.0
-
-        vel_cmd.linear.x = (action[0] + 2.0) / 20.0
-        vel_cmd.angular.z = action[1]
-        self.vel_cmd = [vel_cmd.linear.x, vel_cmd.angular.z]
-        self.pub_cmd_vel.publish(vel_cmd)
-        # print(self.vel_cmd)
-
-        data = None
-        while data is None:
-            try:
-                data = rospy.wait_for_message('scan', LaserScan, timeout=5)
-            except:
-                pass
-
-
-
-#
-        state, done, success = self.getState(data)
-        # state, done, success = self.getState(laser)
-#
-        reward = self.setReward(state, done)
-
-        # 到达目标或者碰撞到障碍物都reset
-        # print(state[-2])
-        return np.array(state), reward, done, success, {}
-
-    def render(self):
-        pass
-
-    def reset(self):
-        rospy.wait_for_service('gazebo/reset_simulation')
-        try:
-            self.reset_proxy()
-        except (rospy.ServiceException) as e:
-            print("gazebo/reset_simulation service call failed")
-
-        data = None
-        while data is None:
-            try:
-                data = rospy.wait_for_message('scan', LaserScan, timeout=5)
-            except:
-                pass
-        # send_goal=None
-
-        if self.initGoal:
-            # while send_goal is None:
-        #         try:
-        #             send_goal = rospy.wait_for_message('/move_base_simple/goal', PoseStamped, timeout=5)
-        #         except:
-        #             pass
-            
-        #     self.goal_x, self.goal_y = send_goal.pose.position.x, send_goal.pose.position.y
-            self.goal_x, self.goal_y = self.respawn_goal.getPosition()
-            self.initGoal = False
-            # print(send_goal)
-            # rospy.loginfo("Goal position : %.1f, %.1f", self.goal_x,
-            #                   self.goal_y)
-        self.vel_cmd = [0., 0.]
-        self.goal_distance = self.getGoalDistace()
-
-      
-        state, done, success = self.getState(data)
-        # print(np.array(state).shape)
-
-        return np.array(state)
-
-
-
diff --git a/preddrl_td3/gazebo_env/respawnGoal.py b/preddrl_td3/gazebo_env/respawnGoal.py
deleted file mode 100644
index 777bc1a..0000000
--- a/preddrl_td3/gazebo_env/respawnGoal.py
+++ /dev/null
@@ -1,241 +0,0 @@
-import rospy
-import random
-import time
-import os
-from gazebo_msgs.srv import SpawnModel, DeleteModel
-from gazebo_msgs.msg import ModelStates
-from geometry_msgs.msg import Pose
-
-
-class Respawn():
-    def __init__(self):
-        self.modelPath = os.path.dirname(os.path.realpath(__file__))
-        self.modelPath = '/home/ros_admin/tf2rl_turtlebot3-master/src/turtlebot3_simulations/turtlebot3_gazebo/models/turtlebot3_square/goal_box/model.sdf'
-        self.f = open(self.modelPath, 'r')
-        self.model = self.f.read()
-        # self.stage = rospy.get_param('/stage_number')
-        self.stage = 2
-        self.goal_position = Pose()
-        # self.init_goal_x = -0.586480
-        # self.init_goal_y = 4.857300
-        self.init_goal_x = 0#1.5#0.5 1.5 0
-        self.init_goal_y = 1#0.2#-1.5 0 -1
-        self.goal_position.position.x = self.init_goal_x
-        self.goal_position.position. y = self.init_goal_y
-        self.modelName = 'goal'
-        self.obstacle_1 = 0.6, 0.6
-        self.obstacle_2 = 0.6, -0.6
-        self.obstacle_3 = -0.6, 0.6
-        self.obstacle_4 = -0.6, -0.6
-        self.test_index = 0
-        self.last_goal_x = self.init_goal_x
-        self.last_goal_y = self.init_goal_y
-        self.last_index = 0
-        self.sub_model = rospy.Subscriber('gazebo/model_states', ModelStates, self.checkModel)
-        self.check_model = False
-        self.index = 0
-
-    def checkModel(self, model):
-        self.check_model = False
-        for i in range(len(model.name)):
-            if model.name[i] == "goal":
-                self.check_model = True
-
-    def respawnModel(self):
-        while True:
-            if not self.check_model:
-                rospy.wait_for_service('gazebo/spawn_sdf_model')
-                spawn_model_prox = rospy.ServiceProxy('gazebo/spawn_sdf_model', SpawnModel)
-                spawn_model_prox(self.modelName, self.model, 'robotos_name_space', self.goal_position, "world")
-                rospy.loginfo("Goal position : %.1f, %.1f", self.goal_position.position.x,
-                              self.goal_position.position.y)
-                break
-            else:
-                pass
-
-    def deleteModel(self):
-        while True:
-            if self.check_model:
-                rospy.wait_for_service('gazebo/delete_model')
-                del_model_prox = rospy.ServiceProxy('gazebo/delete_model', DeleteModel)
-                del_model_prox(self.modelName)
-                break
-            else:
-                pass
-
-    def getPosition(self, position_check=False, delete=False, test=False):
-        # goal_xy_list = {
-        #         [1.5,2.5],[2.5,-0.5]
-        #     }
-        if delete:
-            self.deleteModel()
-
-        if test or self.stage == -1:
-            #第一幅地图
-            # goal_x = [0., -1.0, 0., 1.]
-            # goal_y = [-1.5, 0., 1.5, 0]
-            #第二幅地图静态
-            # goal_x = [3., 1.5, -1., 1.5]
-            # goal_y = [0, 4, 3, 0]
-            #第二幅地图动态
-            # goal_x = [0.0,-2.0,-4.5,-5.0,-6.5,-8.0,-9.0,-9.0,-5.0,-5.0,-2.0,-2.0, 0.0]#-5
-            # goal_y = [3.0, 4.0, 3.0, 1.0, 4.3, 6.0, 8.5, 7.0, 7.0, 5.0, 3.5, 1.0, 0.0]#8.3
-
-            # goal_x = [3.5, 0 ]#-5
-            # goal_y = [4.5, 0]
-
-            goal_x = [3.5, 0  , 4, 0]#-5
-            goal_y = [4.5, 4.5, 0, 0]
-            # goal_x = [3, -3, 0, 3., -3., 0]
-            # goal_y = [-3., -3., 0, 3., 3., 0]
-            # goal_x = [3.5, 0  , 4, 0,  0, 4, 3.5,0]#-5
-            # goal_y = [4.5, 4.5, 0, 0,4.5, 0, 4.5,0]
-
-            # goal_x = [0 ]#-5 
-            # goal_y = [5]
-            # goal_x = [1.5]#-5 
-            # goal_y = [0]
-            # goal_x = [3, -3, 0, 3., -3., 0]
-            # goal_y = [-3., -3., 0, 3., 3., 0]
-            # goal_x = [-5.523510, -6.815110, -4]
-            # goal_y = [4.138770, 5.614350, 7]
-            if self.test_index == len(goal_x):
-                print("end:", time.time())
-            self.goal_position.position.x = goal_x[self.test_index]
-            self.goal_position.position.y = goal_y[self.test_index]
-            # self.goal_position.position.x, self.goal_position.position.y = random.choice(goal_xy_list)
-            self.test_index += 1
-
-        elif self.stage == 0:
-            while position_check:
-                goal_x_list = [0,0]
-                goal_y_list = [5,0]
-
-                # goal_x_list = [0,3,4,3,1]
-                # goal_y_list = [5,4,0,1.5,2]
-                # goal_x_list = [2., 1., 2.5, -2., -3., 2., -2., 0., 1., -1., -3.5, -1., 3.5]
-                # goal_y_list = [0., -1., 2.5, 0., 2., -3.5, -2., -1., 1., 2.5, -3.5, 1.3, 1.5]
-
-                self.index = random.randrange(0, len(goal_x_list))
-                # print(self.index, self.last_index)
-                if self.last_index == self.index:
-                    position_check = True
-                else:
-                    self.last_index = self.index
-                    position_check = False
-
-                self.goal_position.position.x = goal_x_list[self.index]
-                self.goal_position.position.y = goal_y_list[self.index]
-
-        elif self.stage == 2 or self.stage == 3:
-            while position_check:
-                goal_x = random.randrange(-12, 13) / 10.0
-                goal_y = random.randrange(-12, 13) / 10.0
-                if abs(goal_x - self.obstacle_1[0]) <= 0.4 and abs(goal_y - self.obstacle_1[1]) <= 0.4:
-                    position_check = True
-                elif abs(goal_x - self.obstacle_2[0]) <= 0.4 and abs(goal_y - self.obstacle_2[1]) <= 0.4:
-                    position_check = True
-                elif abs(goal_x - self.obstacle_3[0]) <= 0.4 and abs(goal_y - self.obstacle_3[1]) <= 0.4:
-                    position_check = True
-                elif abs(goal_x - self.obstacle_4[0]) <= 0.4 and abs(goal_y - self.obstacle_4[1]) <= 0.4:
-                    position_check = True
-                elif abs(goal_x - 0.0) <= 0.4 and abs(goal_y - 0.0) <= 0.4:
-                    position_check = True
-                else:
-                    position_check = False
-
-                if abs(goal_x - self.last_goal_x) < 1 and abs(goal_y - self.last_goal_y) < 1:
-                    position_check = True
-
-                self.goal_position.position.x = goal_x
-                self.goal_position.position.y = goal_y
-
-        elif self.stage == 1:
-            while position_check:
-                goal_x_list = [1.5, 2.5, -1.5, -0.5, 3.7, 3.5, 1.5, 0., 0.5, 0.5, 3.5, 2.5, 3.5]
-                goal_y_list = [2.5, -0.5, -0.5, 2.5, 3, 1., 4., 4.5, 5., 2.5, -0.5, 1.3, 3.5]
-
-                self.index = random.randrange(0, 13)
-                # print(self.index, self.last_index)
-                if self.last_index == self.index:
-                    position_check = True
-                else:
-                    self.last_index = self.index
-                    position_check = False
-
-                self.goal_position.position.x = goal_x_list[self.index]
-                self.goal_position.position.y = goal_y_list[self.index]
-
-        elif self.stage == 4:
-             while position_check:
-                goal_x_list = [0.6, 1.9, 0.5, 0.2, -0.8, -1, -1.9, 0.5, 0.5, 0, -0.1, -2]
-                goal_y_list = [0, -0.5, -1.9, 1.5, -0.9, 1, 1.1, -1.5, 1.8, -1, 1.6, -0.8]
-
-                self.index = random.randrange(0, 12)
-                # print(self.index, self.last_index)
-                if self.last_index == self.index:
-                    position_check = True
-                else:
-                    self.last_index = self.index
-                    position_check = False
-
-                self.goal_position.position.x = goal_x_list[self.index]
-                self.goal_position.position.y = goal_y_list[self.index]
-        elif self.stage == 5:
-             while position_check:
-                
-                goal_xy_list = [
-                [-1.5, 0.5], [-1.5, 1.5], [-0.5, 0.5], [-0.5, 1.5],
-                [0.5, -0.5], [0.5, -1.5], [2.5, -0.5], [2.5, 0.5],
-                [5.5,-1.5], [5.5,-0.5], [5.5,0.5], [5.5,1.5]
-                ]
-                self.index = random.randrange(0, 12)
-                # print(self.index, self.last_index)
-                if self.last_index == self.index:
-                    position_check = True
-                else:
-                    self.last_index = self.index
-                    position_check = False
-
-                self.goal_position.position.x = goal_xy_list[self.index][0]
-                self.goal_position.position.y = goal_xy_list[self.index][1]
-        else:
-            while position_check:
-                # train_env_1
-                # goal_x_list = [0, 1, 1, -1, -1, -1, -2.5, 0., 2.5, 2.5, -1.5, 2., 0.5, 1.0, -1.0, 1.5, -1.5]
-                # goal_y_list = [2., 1, -1, -1, 1, 1, -1.5, 3.5, 3.5, -1.5, 2, 2., 0.5, 3.5,  3.5, 4.5, 4.5]
-
-                # train_env_2
-                goal_x_list = [2., 1., 2.5, -2., -3., 2., -2., 0., 1., -1., -3.5, -1., 3.5]
-                goal_y_list = [0., -1., 2.5, 0., 2., -3.5, -2., -1., 1., 2.5, -3.5, 1.3, 1.5]
-
-                self.index = random.randrange(0, len(goal_x_list))
-                if self.last_index == self.index:
-                    position_check = True
-                else:
-                    self.last_index = self.index
-                    position_check = False
-
-                self.goal_position.position.x = goal_x_list[self.index]
-                self.goal_position.position.y = goal_y_list[self.index]
-
-        # time.sleep(0.5)
-        self.respawnModel()
-
-        self.last_goal_x = self.goal_position.position.x
-        self.last_goal_y = self.goal_position.position.y
-
-        return self.goal_position.position.x, self.goal_position.position.y
-    # def getPosition(self, position_check=False, delete=False, test=False):
-    #     if delete:
-    #         self.deleteModel()
-    #
-    #
-    #
-    #     # time.sleep(0.5)
-    #     self.respawnModel()
-    #
-    #     self.goal_position.position.x = self.init_goal_x
-    #     self.goal_position.position.y = self.init_goal_y
-    #
-    #     return self.goal_position.position.x, self.goal_position.position.y
\ No newline at end of file
diff --git a/preddrl_td3/script/ddpg.py b/preddrl_td3/script/ddpg.py
deleted file mode 100644
index 191d6a5..0000000
--- a/preddrl_td3/script/ddpg.py
+++ /dev/null
@@ -1,232 +0,0 @@
-import numpy as np
-import tensorflow as tf
-from tensorflow.keras.layers import Dense, concatenate, Conv1D, Flatten, Dropout
-
-from tf2rl.algos.policy_base import OffPolicyAgent
-from tf2rl.misc.target_update_ops import update_target_variables
-from tf2rl.misc.huber_loss import huber_loss
-
-
-class Actor(tf.keras.Model):
-    def __init__(self, state_shape, action_dim, max_action, units=[256, 256], name="Actor"):
-        super().__init__(name=name)
-
-        self.l1 = Dense(256, name="L1")#400 256
-        self.l2 = Dense(256, name="L2")#300 256
-        self.l3 = Dense(action_dim, name="L3")
-        self.l4 = Dense(128, name="L4")
-        self.l5 = Dense(256, name="L5")
-        self.l6 = Conv1D(4,2,padding='same',strides=1,activation='relu',input_shape=(21,1),name="Conv1")
-        self.l7 = Conv1D(4,2,padding='same',strides=1,activation='relu',name="Conv2")
-        self.l8 = Conv1D(8,6,padding='same',strides=1,activation='relu',name="Conv3")
-        self.l9 = Dense(128, name="L9")
-        
-
-        self.l10 = Dense(256, name="L10")
-        self.l11 = Dense(256, name="L11")
-        self.l12 = Dropout(rate=0.2)
-         
-        self.max_action = max_action
-
-        with tf.device("/gpu:0"):
-            self(tf.constant(np.zeros(shape=(1,)+state_shape, dtype=np.float32)))
-
-    def call(self, inputs):
-        #DNN
-        features = tf.nn.relu(self.l1(inputs))
-        features = tf.nn.relu(self.l2(features))
-        features = self.l3(features)
-        action = self.max_action * tf.nn.tanh(features)
-        return action
-
-        #CNN-1 
-        # input_ = tf.expand_dims(inputs,axis=-1)
-        
-        # features = self.l6(input_[:,:20])
-        # # print(features)
-        # # features = self.l7(features)
-        # # print(features)
-        # features = Flatten()(features)
-        
-        # features1 = tf.nn.relu(self.l1(features))
-
-        # state = tf.concat([features1,inputs[:,20:24]],axis=-1)
-        # # print(state)
-        # features =  tf.nn.relu(self.l2(state))
-        # features =  tf.nn.relu(self.l8(features))
-        # features = self.l3(features)
-
-        # CNN-2
-        # input_ = tf.expand_dims(inputs,axis=-1)
-        # # 3 Conv layers
-        # features = self.l6(input_[:,:21])
-        # # features = self.l7(features)
-        # # features = self.l8(features)
-        # print(features.shape) 
-        # features = Flatten()(features)   
-            
-        # features1 = tf.nn.relu(self.l1(features))
-
-        # # DNN layers
-        # vel = tf.nn.relu(self.l10(inputs[:,21:23]))
-        # polor = tf.nn.relu(self.l11(inputs[:,23:25]))
-        # state = tf.concat([features1,vel,polor],axis=-1) #256x3
-        
-        # features = tf.nn.relu(self.l2(state)) #256
-        # features = tf.nn.relu(self.l9(features))#128
-        # # features = self.l12(features)
-        # features = self.l3(features)
-    
-        # action = self.max_action * tf.nn.tanh(features)
-        # return action
-#
-        
-
-class Critic(tf.keras.Model):
-    def __init__(self, state_shape, action_dim, units=[256, 256], name="Critic"):
-        super().__init__(name=name)
-
-        self.l1 = Dense(units[0], name="L1")
-        self.l2 = Dense(units[1], name="L2")
-        self.l3 = Dense(1, name="L3")
-
-        dummy_state = tf.constant(
-            np.zeros(shape=(1,)+state_shape, dtype=np.float32))
-        dummy_action = tf.constant(
-            np.zeros(shape=[1, action_dim], dtype=np.float32))
-        with tf.device("/gpu:0"):
-            self([dummy_state, dummy_action])
-
-    def call(self, inputs):
-        states, actions = inputs
-        features = tf.concat([states, actions], axis=1)
-        features = tf.nn.relu(self.l1(features))
-        features = tf.nn.relu(self.l2(features))
-        features = self.l3(features)
-        return features
-
-
-class DDPG(OffPolicyAgent):
-    def __init__(
-            self,
-            state_shape,
-            action_dim,
-            name="DDPG",
-            max_action=1.,
-            lr_actor=0.001,
-            lr_critic=0.001,
-            actor_units=[400, 300],
-            critic_units=[400, 300],
-            sigma=0.1,
-            tau=0.005,
-            n_warmup=int(1e4),
-            memory_capacity=int(1e6),
-            **kwargs):
-        super().__init__(name=name, memory_capacity=memory_capacity, n_warmup=n_warmup, **kwargs)
-
-        # Define and initialize Actor network
-        self.actor = Actor(state_shape, action_dim, max_action, actor_units)
-        self.actor_target = Actor(
-            state_shape, action_dim, max_action, actor_units)
-        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)
-        update_target_variables(self.actor_target.weights,
-                                self.actor.weights, tau=1.)
-
-        # Define and initialize Critic network
-        self.critic = Critic(state_shape, action_dim, critic_units)
-        self.critic_target = Critic(state_shape, action_dim, critic_units)
-        self.critic_optimizer = tf.keras.optimizers.Adam(
-            learning_rate=lr_critic)
-        update_target_variables(
-            self.critic_target.weights, self.critic.weights, tau=1.)
-
-        # Set hyperparameters
-        self.sigma = sigma
-        self.tau = tau
-
-    def get_action(self, state, test=False, tensor=False):
-        is_single_state = len(state.shape) == 1
-        if not tensor:
-            assert isinstance(state, np.ndarray)
-        state = np.expand_dims(state, axis=0).astype(
-            np.float32) if is_single_state else state
-        action = self._get_action_body(
-            tf.constant(state), self.sigma * (1. - test),
-            tf.constant(self.actor.max_action, dtype=tf.float32))
-        if tensor:
-            return action
-        else:
-            return action.numpy()[0] if is_single_state else action.numpy()
-
-    @tf.function
-    def _get_action_body(self, state, sigma, max_action):
-        with tf.device(self.device):
-            action = self.actor(state)
-            action += tf.random.normal(shape=action.shape,
-                                       mean=0., stddev=sigma, dtype=tf.float32)
-            return tf.clip_by_value(action, -max_action, max_action)
-
-    def train(self, states, actions, next_states, rewards, done, weights=None):
-        if weights is None:
-            weights = np.ones_like(rewards)
-        actor_loss, critic_loss, td_errors = self._train_body(
-            states, actions, next_states, rewards, done, weights)
-
-        if actor_loss is not None:
-            tf.summary.scalar(name=self.policy_name+"/actor_loss",
-                              data=actor_loss)
-        tf.summary.scalar(name=self.policy_name+"/critic_loss",
-                          data=critic_loss)
-
-        return td_errors
-
-    @tf.function
-    def _train_body(self, states, actions, next_states, rewards, done, weights):
-        with tf.device(self.device):
-            with tf.GradientTape() as tape:
-                td_errors = self._compute_td_error_body(
-                    states, actions, next_states, rewards, done)
-                critic_loss = tf.reduce_mean(
-                    huber_loss(td_errors, delta=self.max_grad) * weights)
-
-            critic_grad = tape.gradient(
-                critic_loss, self.critic.trainable_variables)
-            self.critic_optimizer.apply_gradients(
-                zip(critic_grad, self.critic.trainable_variables))
-
-            with tf.GradientTape() as tape:
-                next_action = self.actor(states)
-                actor_loss = -tf.reduce_mean(self.critic([states, next_action]))
-
-            actor_grad = tape.gradient(
-                actor_loss, self.actor.trainable_variables)
-            self.actor_optimizer.apply_gradients(
-                zip(actor_grad, self.actor.trainable_variables))
-
-            # Update target networks
-            update_target_variables(
-                self.critic_target.weights, self.critic.weights, self.tau)
-            update_target_variables(
-                self.actor_target.weights, self.actor.weights, self.tau)
-
-            return actor_loss, critic_loss, td_errors
-
-    def compute_td_error(self, states, actions, next_states, rewards, dones):
-        if isinstance(actions, tf.Tensor):
-            rewards = tf.expand_dims(rewards, axis=1)
-            dones = tf.expand_dims(dones, 1)
-        td_errors = self._compute_td_error_body(
-            states, actions, next_states, rewards, dones)
-        return np.abs(np.ravel(td_errors.numpy()))
-
-    @tf.function
-    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):
-        with tf.device(self.device):
-            not_dones = 1. - dones
-            target_Q = self.critic_target(
-                [next_states, self.actor_target(next_states)])
-            target_Q = rewards + (not_dones * self.discount * target_Q)
-            target_Q = tf.stop_gradient(target_Q)
-            current_Q = self.critic([states, actions])
-            td_errors = target_Q - current_Q
-        return td_errors
diff --git a/preddrl_td3/script/run_td3.py b/preddrl_td3/script/run_td3.py
deleted file mode 100644
index 1dbd835..0000000
--- a/preddrl_td3/script/run_td3.py
+++ /dev/null
@@ -1,45 +0,0 @@
-# import sys
-# sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
-import sys
-sys.path.insert(0, './')
-import gym
-import rospy
-
-from gazebo_env.environment_stage_3_bk import Env
-from td3 import TD3
-from trainer import Trainer
-
-
-# Load = True
-Load = False
-if __name__ == '__main__':
-    parser = Trainer.get_argument()
-    parser = TD3.get_argument(parser)
-
-    args = parser.parse_args()
-    args.model_dir = '/home/ros_admin/tf2rl_ws/results/compare_network/1conv_2dnn_3input_dropout_1'
-
-    print(vars(args))
-    # env = gym.make(args.env_name)
-    # test_env = gym.make(args.env_name)
-    rospy.init_node('turtlebot3_td3_stage_3')
-    env = Env()
-    test_env = Env()
-
-    policy = TD3(
-        state_shape=env.observation_space.shape,
-        action_dim=env.action_space.high.size,
-        gpu=0,
-        memory_capacity=args.memory_capacity,
-        max_action=env.action_space.high[0],
-        batch_size=args.batch_size,
-        actor_units=[400, 300],
-        n_warmup=args.n_warmup)
-    trainer = Trainer(policy, env, args, test_env=test_env)
-    
-    if args.phase=='test':
-        trainer.evaluate_policy(10000)  # 每次测试都会在生成临时文件，要定期处理
-    elif args.phase=='train':
-        trainer()
-    else:
-        print('args.phase not unknown')
diff --git a/preddrl_td3/script/td3.py b/preddrl_td3/script/td3.py
deleted file mode 100644
index 00fa0e0..0000000
--- a/preddrl_td3/script/td3.py
+++ /dev/null
@@ -1,184 +0,0 @@
-import numpy as np
-import tensorflow as tf
-from tensorflow.keras.layers import Dense, Conv1D, Flatten, concatenate
-
-from ddpg import DDPG, Actor
-from tf2rl.misc.target_update_ops import update_target_variables
-from tf2rl.misc.huber_loss import huber_loss
-
-
-class Critic(tf.keras.Model):
-    def __init__(self, state_shape, action_dim, units=[400, 300], name="Critic"):
-        super().__init__(name=name)
-
-        self.l1 = Dense(units[0], name="L1")
-        self.l2 = Dense(units[1], name="L2")
-        self.l3 = Dense(1, name="L3")
-
-        self.l4 = Dense(units[0], name="L4")
-        self.l5 = Dense(units[1], name="L5")
-        self.l6 = Dense(1, name="L6")
-
-        # self.l7 = Conv1D(4,20,padding='same',strides=2,activation='relu',input_shape=(-1,20,1),name="Conv1")
-        # self.l8 = Conv1D(4,20,padding='same',strides=2,activation='relu',input_shape=(-1,20,1),name="Conv2")   
-        # self.l9 = Dense(64, name="L9")
-        # self.l10 = Dense(64, name="L10")
-        # self.l11 = Dense(64, name="L11")
-        # self.l12 = Dense(64, name="L12")
-        # self.l13 = Dense(64, name="L13")
-        # self.l14 = Dense(64, name="L14")
-         
-        
-        dummy_state = tf.constant(
-            np.zeros(shape=(1,)+state_shape, dtype=np.float32))
-        dummy_action = tf.constant(
-            np.zeros(shape=[1, action_dim], dtype=np.float32))
-        with tf.device("/gpu:0"):
-            self([dummy_state, dummy_action])
-            
-
-    def call(self, inputs):
-        states, actions = inputs
-        
-        xu = tf.concat([states, actions], axis=1)
-        # xu_ = tf.expand_dims(xu,axis=-1)
-        #
-        x1 = tf.nn.relu(self.l1(xu))
-        x1 = tf.nn.relu(self.l2(x1))
-        x1 = self.l3(x1)
-
-        x2 = tf.nn.relu(self.l4(xu))
-        x2 = tf.nn.relu(self.l5(x2))
-        x2 = self.l6(x2)
-        #
-
-        # x1 = self.l7(xu_[:,:20])
-        # x1 = Flatten()(x1)
-        # x1 = tf.nn.relu(self.l9(x1))#64
-
-        # x1_2 = tf.nn.relu(self.l1(xu[:,20:22])) #64
-        # x1_3 = tf.nn.relu(self.l2(xu[:,22:24])) #64
-
-        # x1_4 = tf.nn.relu(self.l13(xu[:,24:26])) #64
-
-        # x1_ = concatenate([x1, x1_2, x1_3, x1_4],axis=-1)
-        # x1_1 = tf.nn.relu(self.l11(x1_))
-        # x1_f = self.l3(x1_1)
-
-        # x2 = self.l8(xu_[:,:20])
-        # x2 = Flatten()(x2)
-        # x2 = tf.nn.relu(self.l10(x2)) #64
-
-        # x2_2 = tf.nn.relu(self.l4(xu[:,20:22])) #64
-        # x2_3 = tf.nn.relu(self.l5(xu[:,22:24])) #64
-
-        # x2_4 = tf.nn.relu(self.l14(xu[:,24:26])) #64
-
-        # x2_ = concatenate([x2, x2_2, x2_3, x2_4],axis=-1)
-        # x2_1 = tf.nn.relu(self.l12(x2_))
-        # x2_f = self.l6(x2_1)
-
-        # return x1_f, x2_f
-        return x1, x2
-
-
-class TD3(DDPG):
-    def __init__(
-            self,
-            state_shape,
-            action_dim,
-            name="TD3",
-            actor_update_freq=2,
-            policy_noise=0.2,
-            noise_clip=0.5,
-            actor_units=[400, 300],
-            critic_units=[400, 300],
-            lr_critic=0.001,
-            **kwargs):
-        super().__init__(name=name, state_shape=state_shape, action_dim=action_dim,
-                         actor_units=actor_units, critic_units=critic_units,
-                         lr_critic=lr_critic, **kwargs)
-
-        self.critic = Critic(state_shape, action_dim, critic_units)
-        self.critic_target = Critic(state_shape, action_dim, critic_units)
-        update_target_variables(
-            self.critic_target.weights, self.critic.weights, tau=1.)
-        self.critic_optimizer = tf.keras.optimizers.Adam(
-            learning_rate=lr_critic)
-
-        self._policy_noise = policy_noise
-        self._noise_clip = noise_clip
-
-        self._actor_update_freq = actor_update_freq
-        self._it = tf.Variable(0, dtype=tf.int32)
-
-    @tf.function
-    def _train_body(self, states, actions, next_states, rewards, done, weights):
-        with tf.device(self.device):
-            with tf.GradientTape() as tape:
-                td_error1, td_error2 = self._compute_td_error_body(
-                    states, actions, next_states, rewards, done)
-                critic_loss = tf.reduce_mean(huber_loss(td_error1, delta=self.max_grad) * weights) + \
-                              tf.reduce_mean(huber_loss(td_error2, delta=self.max_grad) * weights)
-
-            critic_grad = tape.gradient(
-                critic_loss, self.critic.trainable_variables)
-            self.critic_optimizer.apply_gradients(
-                zip(critic_grad, self.critic.trainable_variables))
-
-            self._it.assign_add(1)
-            with tf.GradientTape() as tape:
-                next_actions = self.actor(states)
-                actor_loss = - \
-                    tf.reduce_mean(self.critic([states, next_actions]))
-                # actor_loss = - \
-                #     tf.reduce_mean(self.critic([states[:,:20],states[:,20:22],states[:,22:24], next_actions]))
-            remainder = tf.math.mod(self._it, self._actor_update_freq)
-            def optimize_actor():
-                actor_grad = tape.gradient(
-                    actor_loss, self.actor.trainable_variables)
-                return self.actor_optimizer.apply_gradients(
-                    zip(actor_grad, self.actor.trainable_variables))
-
-            tf.cond(pred=tf.equal(remainder, 0), true_fn=optimize_actor, false_fn=tf.no_op)
-            # Update target networks
-            update_target_variables(
-                self.critic_target.weights, self.critic.weights, self.tau)
-            update_target_variables(
-                self.actor_target.weights, self.actor.weights, self.tau)
-
-            return actor_loss, critic_loss, tf.abs(td_error1) + tf.abs(td_error2)
-
-    def compute_td_error(self, states, actions, next_states, rewards, dones):
-        td_errors1, td_errors2 = self._compute_td_error_body(
-            states, actions, next_states, rewards, dones)
-        return np.squeeze(np.abs(td_errors1.numpy()) + np.abs(td_errors2.numpy()))
-
-    @tf.function
-    def _compute_td_error_body(self, states, actions, next_states, rewards, dones):
-        with tf.device(self.device):
-            not_dones = 1. - dones
-
-            # Get noisy action
-            next_action = self.actor_target(next_states)
-            noise = tf.cast(tf.clip_by_value(
-                tf.random.normal(shape=tf.shape(next_action),
-                                 stddev=self._policy_noise),
-                -self._noise_clip, self._noise_clip), tf.float32)
-            next_action = tf.clip_by_value(
-                next_action + noise, -self.actor_target.max_action, self.actor_target.max_action)
-
-            target_Q1, target_Q2 = self.critic_target(
-                [next_states, next_action])
-            # target_Q1, target_Q2 = self.critic_target(
-            #     [next_states[:,:20], next_states[:,20:22], next_states[:,22:24], next_action])
-            
-            target_Q = tf.minimum(target_Q1, target_Q2)
-            target_Q = rewards + (not_dones * self.discount * target_Q)
-            target_Q = tf.stop_gradient(target_Q)
-
-            current_Q1, current_Q2 = self.critic([states, actions])
-            # print(states.shape)
-            # current_Q1, current_Q2 = self.critic([states[:,:20],states[:,20:22],states[:,22:24], actions])
-
-        return target_Q - current_Q1, target_Q - current_Q2
diff --git a/preddrl_td3/script/trainer.py b/preddrl_td3/script/trainer.py
deleted file mode 100644
index 4e6b2d4..0000000
--- a/preddrl_td3/script/trainer.py
+++ /dev/null
@@ -1,337 +0,0 @@
-import os
-import time
-import logging
-import argparse
-
-import numpy as np
-import tensorflow as tf
-from gym.spaces import Box
-
-from tf2rl.experiments.utils import save_path, frames_to_gif
-from tf2rl.misc.get_replay_buffer import get_replay_buffer
-from tf2rl.misc.prepare_output_dir import prepare_output_dir
-from tf2rl.misc.initialize_logger import initialize_logger
-from tf2rl.envs.normalizer import EmpiricalNormalizer
-
-
-if tf.config.experimental.list_physical_devices('GPU'):
-    for cur_device in tf.config.experimental.list_physical_devices("GPU"):
-        print(cur_device)
-        tf.config.experimental.set_memory_growth(cur_device, enable=True)
-
-
-class Trainer:
-    def __init__(
-            self,
-            policy,
-            env,
-            args,
-            test_env=None):
-        self._set_from_args(args)
-        self._policy = policy
-        self._env = env
-        self._test_env = self._env if test_env is None else test_env
-        if self._normalize_obs:
-            assert isinstance(env.observation_space, Box)
-            self._obs_normalizer = EmpiricalNormalizer(
-                shape=env.observation_space.shape)
-
-        # prepare log directory
-        self._output_dir = prepare_output_dir(
-            args=args, user_specified_dir=self._logdir,
-            suffix="{}_{}".format(self._policy.policy_name, args.dir_suffix))
-        self.logger = initialize_logger(
-            logging_level=logging.getLevelName(args.logging_level),
-            output_dir=self._output_dir)
-
-        if args.evaluate:
-            assert args.model_dir is not None
-        self._set_check_point(args.model_dir)
-
-        # prepare TensorBoard output
-        self.writer = tf.summary.create_file_writer(self._output_dir)
-        self.writer.set_as_default()
-
-    def _set_check_point(self, model_dir):
-        # Save and restore model
-        self._checkpoint = tf.train.Checkpoint(policy=self._policy)
-        self.checkpoint_manager = tf.train.CheckpointManager(
-            self._checkpoint, directory=self._output_dir, max_to_keep=5)
-
-        if model_dir is not None:
-            assert os.path.isdir(model_dir)
-            self._latest_path_ckpt = tf.train.latest_checkpoint(model_dir)
-            self._checkpoint.restore(self._latest_path_ckpt)
-            self.logger.info("Restored {}".format(self._latest_path_ckpt))
-
-    def __call__(self):
-        total_steps = 0
-        tf.summary.experimental.set_step(total_steps)
-        episode_steps = 0
-        episode_return = 0
-        episode_start_time = time.perf_counter()
-        n_episode = 1
-#for success rate
-        episode_success = 0
-#
-        replay_buffer = get_replay_buffer(
-            self._policy, self._env, self._use_prioritized_rb,
-            self._use_nstep_rb, self._n_step)
-
-# separate input (laser scan, vel, polor)
-#
-        # obs = self._env.reset()
-        obs = self._env.reset()
-#
-        while total_steps < self._max_steps:
-            if total_steps < self._policy.n_warmup:
-                action = self._env.action_space.sample()
-            else:
-                action = self._policy.get_action(obs)
-
-            next_obs, reward, done, success, _ = self._env.step(action)
-            if self._show_progress:
-                self._env.render()
-            episode_steps += 1
-            episode_return += reward
-            total_steps += 1
-            tf.summary.experimental.set_step(total_steps)
-
-            done_flag = done
-            if hasattr(self._env, "_max_episode_steps") and \
-                    episode_steps == self._env._max_episode_steps:
-                done_flag = False
-            replay_buffer.add(obs=obs, act=action,
-                              next_obs=next_obs, rew=reward, done=done_flag)
-            obs = next_obs
-#for success rate
-            if done or episode_steps == self._episode_max_steps or success:
-
-                if success and total_steps > self._policy.n_warmup: 
-                    episode_success += 1
-                if total_steps > self._policy.n_warmup:    
-                    n_episode += 1
-                fps = episode_steps / (time.perf_counter() - episode_start_time)
-                self.logger.info("Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}".format(
-                    n_episode, total_steps, episode_steps, episode_return, fps))
-                tf.summary.scalar(
-                    name="Common/training_return", data=episode_return)
-#                    
-                success_rate =episode_success/n_episode
-                tf.summary.scalar(
-                    name="Common/success rate", data=success_rate)
-#
-                episode_steps = 0
-                episode_return = 0
-                episode_start_time = time.perf_counter()
-
-                  
-
-                if done or episode_steps == self._episode_max_steps:
-                    obs = self._env.reset()
-# # original
-            # if done or episode_steps == self._episode_max_steps:
-            #     obs = self._env.reset()
-
-            #     n_episode += 1
-            #     fps = episode_steps / (time.time() - episode_start_time)
-            #     self.logger.info("Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}".format(
-            #         n_episode, total_steps, episode_steps, episode_return, fps))
-            #     tf.summary.scalar(
-            #         name="Common/training_return", data=episode_return)
-
-            #     episode_steps = 0
-            #     episode_return = 0
-            #     episode_start_time = time.time()                  
-# # #                    
-
-            if total_steps < self._policy.n_warmup:
-                continue
-
-            if total_steps % self._policy.update_interval == 0:
-                samples = replay_buffer.sample(self._policy.batch_size)
-                with tf.summary.record_if(total_steps % self._save_summary_interval == 0):
-                    self._policy.train(
-                        samples["obs"], samples["act"], samples["next_obs"],
-                        samples["rew"], np.array(samples["done"], dtype=np.float32),
-                        None if not self._use_prioritized_rb else samples["weights"])
-                if self._use_prioritized_rb:
-                    td_error = self._policy.compute_td_error(
-                        samples["obs"], samples["act"], samples["next_obs"],
-                        samples["rew"], np.array(samples["done"], dtype=np.float32))
-                    replay_buffer.update_priorities(
-                        samples["indexes"], np.abs(td_error) + 1e-6)
-
-            if total_steps % self._test_interval == 0:
-                avg_test_return = self.evaluate_policy(total_steps)
-                self.logger.info("Evaluation Total Steps: {0: 7} Average Reward {1: 5.4f} over {2: 2} episodes".format(
-                    total_steps, avg_test_return, self._test_episodes))
-                tf.summary.scalar(
-                    name="Common/average_test_return", data=avg_test_return)
-                tf.summary.scalar(name="Common/fps", data=fps)
-                self.writer.flush()
-
-            if total_steps % self._save_model_interval == 0:
-                self.checkpoint_manager.save()
-
-        tf.summary.flush()
-
-    def evaluate_policy_continuously(self):
-        """
-        Periodically search the latest checkpoint, and keep evaluating with the latest model until user kills process.
-        """
-        if self._model_dir is None:
-            self.logger.error("Please specify model directory by passing command line argument `--model-dir`")
-            exit(-1)
-
-        self.evaluate_policy(total_steps=0)
-        while True:
-            latest_path_ckpt = tf.train.latest_checkpoint(self._model_dir)
-            if self._latest_path_ckpt != latest_path_ckpt:
-                self._latest_path_ckpt = latest_path_ckpt
-                self._checkpoint.restore(self._latest_path_ckpt)
-                self.logger.info("Restored {}".format(self._latest_path_ckpt))
-            self.evaluate_policy(total_steps=0)
-
-    def evaluate_policy(self, total_steps):
-        tf.summary.experimental.set_step(total_steps)
-        if self._normalize_obs:
-            self._test_env.normalizer.set_params(
-                *self._env.normalizer.get_params())
-        avg_test_return = 0.
-        if self._save_test_path:
-            replay_buffer = get_replay_buffer(
-                self._policy, self._test_env, size=self._episode_max_steps)
-        for i in range(self._test_episodes):
-            episode_return = 0.
-            frames = []
-# for seperate
-            # obs = self._test_env.reset()
-            obs = self._test_env.reset()
-#
-            for _ in range(self._episode_max_steps):
-                action = self._policy.get_action(obs, test=True)
-                next_obs, reward, done, success, _ = self._test_env.step(action)
-                if self._save_test_path:
-                    replay_buffer.add(obs=obs, act=action,
-                                      next_obs=next_obs, rew=reward, done=done)
-
-                if self._save_test_movie:
-                    frames.append(self._test_env.render(mode='rgb_array'))
-                elif self._show_test_progress:
-                    self._test_env.render()
-                episode_return += reward
-                obs = next_obs
-                if done:
-                    break
-            prefix = "step_{0:08d}_epi_{1:02d}_return_{2:010.4f}".format(
-                total_steps, i, episode_return)
-            if self._save_test_path:
-                save_path(replay_buffer._encode_sample(np.arange(self._episode_max_steps)),
-                          os.path.join(self._output_dir, prefix + ".pkl"))
-                replay_buffer.clear()
-            if self._save_test_movie:
-                frames_to_gif(frames, prefix, self._output_dir)
-            avg_test_return += episode_return
-        if self._show_test_images:
-            images = tf.cast(
-                tf.expand_dims(np.array(obs).transpose(2, 0, 1), axis=3),
-                tf.uint8)
-            tf.summary.image('train/input_img', images,)
-        return avg_test_return / self._test_episodes
-
-    def _set_from_args(self, args):
-        # experiment settings
-        self._max_steps = args.max_steps
-        self._episode_max_steps = args.episode_max_steps \
-            if args.episode_max_steps is not None \
-            else args.max_steps
-        self._n_experiments = args.n_experiments
-        self._show_progress = args.show_progress
-        self._save_model_interval = args.save_model_interval
-        self._save_summary_interval = args.save_summary_interval
-        self._normalize_obs = args.normalize_obs
-        self._logdir = args.logdir
-        self._model_dir = args.model_dir
-        # replay buffer
-        self._use_prioritized_rb = args.use_prioritized_rb
-        self._use_nstep_rb = args.use_nstep_rb
-        self._n_step = args.n_step
-        # test settings
-        self._test_interval = args.test_interval
-        self._show_test_progress = args.show_test_progress
-        self._test_episodes = args.test_episodes
-        self._save_test_path = args.save_test_path
-        self._save_test_movie = args.save_test_movie
-        self._show_test_images = args.show_test_images
-
-    @staticmethod
-    def get_argument(parser=None):
-        if parser is None:
-            parser = argparse.ArgumentParser(conflict_handler='resolve')
-        # experiment settings
-        parser.add_argument('--max-steps', type=int, default=int(1e6),
-                            help='Maximum number steps to interact with env.')
-        parser.add_argument('--episode-max-steps', type=int, default=int(1e3),
-                            help='Maximum steps in an episode')
-        parser.add_argument('--n-experiments', type=int, default=1,
-                            help='Number of experiments')
-        parser.add_argument('--show-progress', action='store_true',
-                            help='Call `render` in training process')
-        parser.add_argument('--save-model-interval', type=int, default=int(1e4),
-                            help='Interval to save model')
-        parser.add_argument('--save-summary-interval', type=int, default=int(1e3),
-                            help='Interval to save summary')
-        parser.add_argument('--model-dir', type=str, default=None,
-                            help='Directory to restore model')
-        parser.add_argument('--dir-suffix', type=str, default='',
-                            help='Suffix for directory that contains results')
-        parser.add_argument('--normalize-obs', action='store_true',
-                            help='Normalize observation')
-        parser.add_argument('--logdir', type=str, default='results',
-                            help='Output directory')
-        # test settings
-        parser.add_argument('--evaluate', action='store_true',
-                            help='Evaluate trained model')
-        parser.add_argument('--test-interval', type=int, default=int(1e6),
-                            help='Interval to evaluate trained model')
-        parser.add_argument('--show-test-progress', action='store_true',
-                            help='Call `render` in evaluation process')
-        parser.add_argument('--test-episodes', type=int, default=5,
-                            help='Number of episodes to evaluate at once')
-        parser.add_argument('--save-test-path', action='store_true',
-                            help='Save trajectories of evaluation')
-        parser.add_argument('--show-test-images', action='store_true',
-                            help='Show input images to neural networks when an episode finishes')
-        parser.add_argument('--save-test-movie', action='store_true',
-                            help='Save rendering results')
-        # replay buffer
-        parser.add_argument('--use-prioritized-rb', action='store_true',
-                            help='Flag to use prioritized experience replay')
-        parser.add_argument('--use-nstep-rb', action='store_true',
-                            help='Flag to use nstep experience replay')
-        parser.add_argument('--n-step', type=int, default=4,
-                            help='Number of steps to look over')
-        # others
-        parser.add_argument('--logging-level', choices=['DEBUG', 'INFO', 'WARNING'],
-                            default='INFO', help='Logging level')
-
-        parser.add_argument('--phase', default='train', 
-                            help='train or test')
-        parser.add_argument('--test_episodes', default=50, type=int, 
-                            help='test episodes during testing')
-        parser.add_argument('--episode_max_steps', default=1e4, type=int, 
-                            help='max episodes during testing')
-        parser.add_argument('--show_test_progress', action='store_true', default=False, 
-                            help='show progress during testing')
-        parser.add_argument('--save_model_interval', default=1e10, type=int, 
-                            help='Save model interval' )
-
-        parser.add_argument('--batch_size', default=100, 
-                            help='Training and test batch_size')
-        parser.add_argument('--n_warmup', default=3000, 
-                            help='Number of warmup iterations')
-        parser.add_argument('--max_steps', default=50000,
-                            help='Max steps for training')        
-
-        return parser
diff --git a/preddrl_tracker/src/pedestrian_state_publisher.py b/preddrl_tracker/src/pedestrian_state_publisher.py
index a364365..062f310 100644
--- a/preddrl_tracker/src/pedestrian_state_publisher.py
+++ b/preddrl_tracker/src/pedestrian_state_publisher.py
@@ -62,7 +62,7 @@ def create_actor_msg(nodes, t):
 
         x, y, vx, vy, ax, ay = node.points_at(t)
 
-        print(t, node.id, x, y, vx, vy, ax, ay)
+        # print(t, node.id, x, y, vx, vy, ax, ay)
 
         theta = math.atan2(vy, vx) # radians
 
